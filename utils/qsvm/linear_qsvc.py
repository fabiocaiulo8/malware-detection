from sklearn.base import BaseEstimator
import numpy as np
import torch
import pennylane as qml
from pennylane.templates import AngleEmbedding, StronglyEntanglingLayers
from torch.nn.functional import relu
from torch.optim import Adam

class LinearQSVC(BaseEstimator):

    def __init__(self, layers=2, steps=100, batch_size=32, learning_rate=0.01, random_state=42, classes_=[-1, 1]):
        self.layers = layers
        self.steps = steps
        self.batch_size = batch_size
        self.learning_rate = learning_rate

        np.random.seed(random_state)
        torch.manual_seed(random_state)
        self.classes_ = classes_
        if len(self.classes_) != 2:
            raise RuntimeError('Only binary classification is supported')

        self.qubits = None
        self.parameters = None
        self.bias = None

    def get_params(self, deep=True):
        return {
            'layers': self.layers,
            'steps': self.steps,
            'batch_size': self.batch_size,
            'learning_rate': self.learning_rate
        }

    def set_params(self, **params):
        for parameter, value in params.items():
            setattr(self, parameter, value)
        return self

    def _quantum_model(self, x):
        """
        Variational Quantum Model
        """
        @qml.qnode(qml.device('lightning.qubit', self.qubits), diff_method='adjoint')
        def quantum_circuit(x):
            AngleEmbedding(x, range(self.qubits)) # embedding
            StronglyEntanglingLayers(self.parameters, range(self.qubits)) # trainable measurement
            return qml.expval(qml.PauliZ(0))
        return quantum_circuit(x) + self.bias # adding a bias

    @staticmethod
    def _hinge_loss(predictions, targets):
        """
        Hinge Loss Implementation
        """
        all_ones = torch.ones_like(targets)
        hinge_loss = all_ones - predictions * targets
        # Since the max(0, x) function is not differentiable, use the mathematically equivalent relu instead
        return relu(hinge_loss)

    def fit(self, X, y):
        """
        Quantum Model Training
        """
        self.qubits = X.shape[1]
        initial_parameters = np.random.random([self.layers, self.qubits, 3])
        self.parameters = torch.tensor(initial_parameters, requires_grad=True)
        self.bias = torch.tensor(0.0)

        optimiser = Adam([self.parameters, self.bias], self.learning_rate)

        for i in range(self.steps):
            batch_indices = np.random.choice(len(X), self.batch_size)

            X_batch = torch.tensor(X[batch_indices])
            y_batch = torch.tensor(y[batch_indices])

            def closure():
                optimiser.zero_grad()
                predictions = torch.stack([self._quantum_model(x) for x in X_batch])
                loss = torch.mean(self._hinge_loss(predictions, y_batch))
                loss.backward()
                return loss

            optimiser.step(closure)

        return self

    def predict(self, X):
        """
        Quantum Model Predictions
        """
        if self.qubits is None:
            raise RuntimeError('Model has not been trained, please call fit() before predict()')
        predictions = []
        for x in X:
            x = torch.tensor(x)
            prediction = self._quantum_model(x).detach().numpy().item()
            predictions.append(self.classes_[1] if prediction > 0 else self.classes_[0])
        return np.array(predictions)

    def predict_proba(self, X):
        """
        Quantum Model Confidences
        """
        if self.qubits is None:
            raise RuntimeError('Model has not been trained, please call fit() before predict()')
        confidences = []
        for x in X:
            x = torch.tensor(x)
            prediction = self._quantum_model(x)
            confidence = torch.sigmoid(prediction).detach().numpy().item()
            confidences.append([1-confidence, confidence])
        return np.array(confidences)
